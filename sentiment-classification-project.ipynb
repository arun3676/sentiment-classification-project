{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51433d5e-f639-41d9-810b-48155e19b448",
   "metadata": {},
   "source": [
    "Sentiment Classification Project\n",
    "\n",
    "Project Overview\n",
    "\n",
    "This project demonstrates how to fine-tune a pre-trained model, DistilBERT, to classify text as either positive or negative sentiment. The process involves using a small dataset and optimizing the workflow to work on devices with limited resources, such as a laptop without a dedicated GPU.\n",
    "\n",
    "Steps to Complete the Project\n",
    "\n",
    "Step 1: Understanding the Goal\n",
    "\n",
    "The aim is to take a sentence and predict whether it has a positive or negative sentiment.\n",
    "\n",
    "For example, \"I love this movie\" is positive, while \"I hate this product\" is negative.\n",
    "\n",
    "Step 2: Preparing the Dataset\n",
    "\n",
    "A small dataset in CSV format is used with two columns:\n",
    "\n",
    "text: The sentence to analyze.\n",
    "\n",
    "label: The sentiment (1 for positive, 0 for negative).\n",
    "\n",
    "Example:\n",
    "\n",
    "text,label\n",
    "I love this movie,1\n",
    "This film was terrible,0\n",
    "\n",
    "Step 3: Tokenization\n",
    "\n",
    "Sentences are converted into a format that the machine learning model can understand.\n",
    "\n",
    "Tokenization breaks down sentences into smaller parts (like words) and assigns numeric IDs to them.\n",
    "\n",
    "Step 4: Fine-Tuning the Model\n",
    "\n",
    "DistilBERT, a smaller version of BERT, is fine-tuned on the dataset.\n",
    "\n",
    "The fine-tuning process involves training the model to understand the relationship between the text and its label.\n",
    "\n",
    "Step 5: Evaluating the Model\n",
    "\n",
    "After training, the model is tested on new data to check its accuracy and performance.\n",
    "\n",
    "Metrics like accuracy, precision, and F1 score are calculated to ensure reliability.\n",
    "\n",
    "Step 6: Saving and Using the Model\n",
    "\n",
    "The trained model is saved locally and can be used to predict the sentiment of new sentences.\n",
    "\n",
    "For example: \"This is amazing!\" â†’ Predicted as Positive.\n",
    "\n",
    "Errors Encountered and Solutions\n",
    "\n",
    "Error 1: PyTorch Not Installed\n",
    "\n",
    "Issue: The error \"PyTorch needs to be installed to be able to return PyTorch tensors\" occurred.\n",
    "\n",
    "Solution: Installed PyTorch using the command:\n",
    "\n",
    "pip install torch\n",
    "\n",
    "Error 2: Undefined tokenized_dataset\n",
    "\n",
    "Issue: The tokenized_dataset variable was used before being defined, causing a NameError.\n",
    "\n",
    "Solution: Ensured the dataset was properly tokenized and assigned to tokenized_dataset before its usage in the training script.\n",
    "\n",
    "These issues were tackled step-by-step, ensuring the project ran smoothly on a system with limited hardware resources.\n",
    "\n",
    "Tools Used\n",
    "\n",
    "Python: The programming language for the project.\n",
    "\n",
    "Hugging Face Transformers: For working with pre-trained models like DistilBERT.\n",
    "\n",
    "PyTorch: For training and handling the model.\n",
    "\n",
    "Scikit-learn: For evaluation metrics.\n",
    "\n",
    "Datasets Library: For loading and processing the data.\n",
    "\n",
    "Key Learnings\n",
    "\n",
    "How to fine-tune a pre-trained model for text classification.\n",
    "\n",
    "Working with limited hardware resources.\n",
    "\n",
    "Using evaluation metrics to measure model performance.\n",
    "\n",
    "This project is an excellent starting point for anyone interested in learning about natural language processing (NLP) and machine learning. By following these steps, you can build a simple yet effective sentiment analysis tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404f678-f5ff-482a-a428-fbe013ab6154",
   "metadata": {},
   "source": [
    "Step 1: Import Libraries\n",
    "\n",
    "Markdown Explanation:\n",
    "We start by importing the necessary Python libraries:\n",
    "\n",
    "transformers: For the pretrained model and tokenizer.\n",
    "\n",
    "datasets: To load and preprocess the dataset. \n",
    "\n",
    "torch: For handling tensors and model training on the CPU. \n",
    "\n",
    "numpy: For numerical computations. \n",
    "\n",
    "sklearn: For evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0feb95b-1dfe-4faf-ab62-4ceecf48dbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl (442 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 4.2/25.1 MB 20.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.9/25.1 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 11.5/25.1 MB 19.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 15.7/25.1 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.8/25.1 MB 20.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Installing collected packages: xxhash, tqdm, requests, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-18.1.0 requests-2.32.3 tqdm-4.67.1 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbded73-6435-4561-8257-39e415278c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91ff8da-92b7-48e3-a453-869dc1efd8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dba179-517d-405e-a63c-422020a83a7e",
   "metadata": {},
   "source": [
    "Step 2: Load and Prepare the Dataset\n",
    "Markdown Explanation:\n",
    "\n",
    "We'll use a small custom dataset saved as a CSV file with two columns: text and label.\n",
    "\n",
    "The datasets library makes it easy to load and preprocess datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90914d6e-ae53-422f-812f-e7595327a819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690ae74c1d1c4ed5a5d6b85f2f6c1ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331258352cf04d8ea0d8d6150643831b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I love this movie', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Path to the dataset CSV file\n",
    "data_files = {\"train\": \"C:/Users/ARUN/Downloads/AI Engineering projects/PROJECT1 BERT/sentiment_data.csv\", \n",
    "              \"validation\": \"C:/Users/ARUN/Downloads/AI Engineering projects/PROJECT1 BERT/sentiment_data.csv\"}\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "# Check the first few rows\n",
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffce615-33d9-4b27-b6c8-781f6ff371a3",
   "metadata": {},
   "source": [
    "Step 3: Tokenize the Data\n",
    "Markdown Explanation:\n",
    "\n",
    "Pretrained models like DistilBERT require tokenized input (convert text into numeric IDs).\n",
    "\n",
    "We'll use the DistilBertTokenizer for this purpose.\n",
    "The map function applies the tokenizer to each row of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd82f69-e515-400f-8de4-e47d02019db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 16.6 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\ARUN\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413fe90b-6239-411d-8070-39b439882247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py): started\n",
      "  Building wheel for pytorch (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [6 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\ARUN\\AppData\\Local\\Temp\\pip-install-9te9hkmf\\pytorch_a6e3a938fe1348ccb83db4beac5864d5\\setup.py\", line 15, in <module>\n",
      "      raise Exception(message)\n",
      "  Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pytorch\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pytorch)\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d090c518-3687-4b6a-86bf-56dbdf25c838",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "PyTorch needs to be installed to be able to return PyTorch tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Format the dataset for PyTorch\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtokenized_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\dataset_dict.py:598\u001b[0m, in \u001b[0;36mDatasetDict.set_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 598\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mformat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2531\u001b[0m, in \u001b[0;36mDataset.set_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m   2529\u001b[0m \u001b[38;5;66;03m# Check that the format_type and format_kwargs are valid and make it possible to have a Formatter\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m get_format_type_from_alias(\u001b[38;5;28mtype\u001b[39m)\n\u001b[1;32m-> 2531\u001b[0m \u001b[43mget_formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mformat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;66;03m# Check filter column\u001b[39;00m\n\u001b[0;32m   2534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(columns, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\__init__.py:133\u001b[0m, in \u001b[0;36mget_formatter\u001b[1;34m(format_type, **format_kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _FORMAT_TYPES[format_type](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_type \u001b[38;5;129;01min\u001b[39;00m _FORMAT_TYPES_ALIASES_UNAVAILABLE:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _FORMAT_TYPES_ALIASES_UNAVAILABLE[format_type]\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFormat type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(_FORMAT_TYPES\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Format the dataset for PyTorch\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtokenized_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\dataset_dict.py:598\u001b[0m, in \u001b[0;36mDatasetDict.set_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 598\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mformat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2531\u001b[0m, in \u001b[0;36mDataset.set_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m   2529\u001b[0m \u001b[38;5;66;03m# Check that the format_type and format_kwargs are valid and make it possible to have a Formatter\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m get_format_type_from_alias(\u001b[38;5;28mtype\u001b[39m)\n\u001b[1;32m-> 2531\u001b[0m \u001b[43mget_formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mformat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;66;03m# Check filter column\u001b[39;00m\n\u001b[0;32m   2534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(columns, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\__init__.py:133\u001b[0m, in \u001b[0;36mget_formatter\u001b[1;34m(format_type, **format_kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _FORMAT_TYPES[format_type](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_type \u001b[38;5;129;01min\u001b[39;00m _FORMAT_TYPES_ALIASES_UNAVAILABLE:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _FORMAT_TYPES_ALIASES_UNAVAILABLE[format_type]\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFormat type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(_FORMAT_TYPES\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[1;31m[... skipping similar frames: <module> at line 12 (1 times), get_formatter at line 133 (1 times), InteractiveShell.run_code at line 3553 (1 times), DatasetDict.set_format at line 598 (1 times), Dataset.set_format at line 2531 (1 times), fingerprint_transform.<locals>._fingerprint.<locals>.wrapper at line 442 (1 times)]\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Format the dataset for PyTorch\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtokenized_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\dataset_dict.py:598\u001b[0m, in \u001b[0;36mDatasetDict.set_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 598\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mformat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2531\u001b[0m, in \u001b[0;36mDataset.set_format\u001b[1;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[0;32m   2529\u001b[0m \u001b[38;5;66;03m# Check that the format_type and format_kwargs are valid and make it possible to have a Formatter\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m get_format_type_from_alias(\u001b[38;5;28mtype\u001b[39m)\n\u001b[1;32m-> 2531\u001b[0m \u001b[43mget_formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mformat_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;66;03m# Check filter column\u001b[39;00m\n\u001b[0;32m   2534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(columns, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\__init__.py:133\u001b[0m, in \u001b[0;36mget_formatter\u001b[1;34m(format_type, **format_kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _FORMAT_TYPES[format_type](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_type \u001b[38;5;129;01min\u001b[39;00m _FORMAT_TYPES_ALIASES_UNAVAILABLE:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _FORMAT_TYPES_ALIASES_UNAVAILABLE[format_type]\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFormat type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(_FORMAT_TYPES\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: PyTorch needs to be installed to be able to return PyTorch tensors."
     ]
    }
   ],
   "source": [
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Format the dataset for PyTorch\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f55f1723-ee25-4265-8894-ba2a42569779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "618d9f3c-0045-49a6-9366-8cb0ad419b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1\n",
      "Uninstalling torch-2.5.1:\n",
      "  Successfully uninstalled torch-2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ARUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~orch'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8831f8b9-14e6-4517-8c5a-9c4b1c40536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387488ef-3f1c-4658-a68b-a18c02d99996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ac047e-3b60-4a72-8251-e3c38d0d42c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(1), 'input_ids': tensor([ 101, 1045, 2293, 2023, 3185,  102,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "# Define the path to the CSV file\n",
    "data_files = {\"train\": \"C:/Users/ARUN/Downloads/AI Engineering projects/PROJECT1 BERT/sentiment_data.csv\",\n",
    "              \"validation\": \"C:/Users/ARUN/Downloads/AI Engineering projects/PROJECT1 BERT/sentiment_data.csv\"}\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Format the dataset for PyTorch\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Print the first example to verify\n",
    "print(tokenized_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b987db-ee4f-44ee-9ea1-eac9250e62eb",
   "metadata": {},
   "source": [
    "Step 4: Load Pretrained Model\n",
    "Markdown Explanation:\n",
    "\n",
    "We'll use DistilBertForSequenceClassification, a smaller and faster version of BERT optimized for your CPU.\n",
    "The num_labels parameter specifies the number of output classes (2 for binary classification: positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a3cd8d-208a-403c-be1d-b3957f159ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Load the DistilBERT model for binary classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da500fbd-29c1-46c8-813c-8e8f2d70af9d",
   "metadata": {},
   "source": [
    "Step 5: Define Training Arguments\n",
    "Markdown Explanation:\n",
    "\n",
    "TrainingArguments specifies how training should be done (e.g., number of epochs, batch size, logging).\n",
    "Adjusted for your device:\n",
    "\n",
    "Small batch size (per_device_train_batch_size=2) to fit in 8GB RAM.\n",
    "Fewer epochs (num_train_epochs=2) for quicker runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61868be8-a46e-4587-abbe-88bd7e89f009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.11.17)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1178f128-63db-4f11-a9b6-72dbc4788415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-kerasNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ARUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~ensorflow'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.19,>=2.18 (from tf-keras)\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.65.5)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\arun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 13.4 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl (390.2 MB)\n",
      "   ---------------------------------------- 0.0/390.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 8.1/390.2 MB 42.1 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 10.2/390.2 MB 45.7 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 19.4/390.2 MB 32.3 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 32.0/390.2 MB 39.1 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 43.0/390.2 MB 42.1 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 56.9/390.2 MB 46.5 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 72.9/390.2 MB 50.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 87.3/390.2 MB 53.1 MB/s eta 0:00:06\n",
      "   ---------- ---------------------------- 102.8/390.2 MB 55.6 MB/s eta 0:00:06\n",
      "   ----------- --------------------------- 110.1/390.2 MB 53.3 MB/s eta 0:00:06\n",
      "   ------------ -------------------------- 124.5/390.2 MB 54.9 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 138.9/390.2 MB 56.2 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 153.6/390.2 MB 57.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 168.8/390.2 MB 58.0 MB/s eta 0:00:04\n",
      "   ------------------ -------------------- 185.9/390.2 MB 59.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 201.1/390.2 MB 60.3 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 217.8/390.2 MB 61.6 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 232.8/390.2 MB 62.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 246.2/390.2 MB 62.5 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 258.7/390.2 MB 62.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 273.9/390.2 MB 68.1 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 278.9/390.2 MB 66.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 292.6/390.2 MB 65.5 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 298.6/390.2 MB 64.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 312.7/390.2 MB 64.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 323.0/390.2 MB 63.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 336.1/390.2 MB 62.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 350.2/390.2 MB 62.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 364.4/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 372.8/390.2 MB 62.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  386.9/390.2 MB 62.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.2 MB 62.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.2/390.2 MB 43.2 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 3.7/5.5 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 16.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard, tensorflow-intel, tensorflow, tf-keras\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow-intel 2.17.0\n",
      "    Uninstalling tensorflow-intel-2.17.0:\n",
      "      Successfully uninstalled tensorflow-intel-2.17.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "Successfully installed tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-intel-2.18.0 tf-keras-2.18.0\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed28d5b2-ff2c-4535-aa30-e032914f8ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ARUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',               # Directory to save model checkpoints\n",
    "    num_train_epochs=2,                 # Number of epochs\n",
    "    per_device_train_batch_size=2,      # Batch size per device\n",
    "    per_device_eval_batch_size=2,       # Batch size for evaluation\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",              # Save model at each epoch\n",
    "    logging_dir='logs',                 # Directory for logs\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    load_best_model_at_end=True,        # Load the best model at the end of training\n",
    "    seed=42                             # Seed for reproducibility\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5fea61-8268-4da3-9d93-da50ac0ba865",
   "metadata": {},
   "source": [
    "Step 6: Define Metrics for Evaluation\n",
    "Markdown Explanation:\n",
    "\n",
    "To evaluate the model, we define metrics like accuracy, precision, recall, and F1 score using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7024ef8d-b93b-4230-a18f-4c3e6295852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b8343-bb0d-4d7a-823f-42d9516f7e27",
   "metadata": {},
   "source": [
    "Step 7: Train the Model\n",
    "Markdown Explanation:\n",
    "\n",
    "Use the Hugging Face Trainer API to simplify the training loop.\n",
    "Pass the model, training arguments, datasets, and metrics to the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c4954f-1f86-4f44-996a-c4bcfe7211b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ARUN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m      7\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      8\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m,               \u001b[38;5;66;03m# Directory to save model checkpoints\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,                 \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m                             \u001b[38;5;66;03m# Seed for reproducibility\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize Trainer\u001b[39;00m\n\u001b[0;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                            \u001b[38;5;66;03m# The model you defined\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                     \u001b[38;5;66;03m# The training arguments\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_dataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# The tokenized training dataset\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# The tokenized validation dataset\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics         \u001b[38;5;66;03m# The metrics function\u001b[39;00m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the DistilBERT model for binary classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',               # Directory to save model checkpoints\n",
    "    num_train_epochs=2,                 # Number of epochs\n",
    "    per_device_train_batch_size=2,      # Batch size per device\n",
    "    per_device_eval_batch_size=2,       # Batch size for evaluation\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",              # Save model at each epoch\n",
    "    logging_dir='logs',                 # Directory for logs\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    load_best_model_at_end=True,        # Load the best model at the end of training\n",
    "    seed=42                             # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                            # The model you defined\n",
    "    args=training_args,                     # The training arguments\n",
    "    train_dataset=tokenized_dataset['train'],  # The tokenized training dataset\n",
    "    eval_dataset=tokenized_dataset['validation'],  # The tokenized validation dataset\n",
    "    compute_metrics=compute_metrics         # The metrics function\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e84645-3f05-4518-b820-aa38eab4397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.622505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.6934785842895508, metrics={'train_runtime': 25.9396, 'train_samples_per_second': 0.386, 'train_steps_per_second': 0.231, 'total_flos': 165584248320.0, 'train_loss': 0.6934785842895508, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load the dataset\n",
    "data_files = {\"train\": \"C:/Users/ARUN/Downloads/AI Engineering projects/PROJECT1 BERT/sentiment_data.csv\",\n",
    "              \"validation\": \"C:/Users/ARUN/Downloads/AI Engineering projects/PROJECT1 BERT/sentiment_data.csv\"}\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Format the dataset for PyTorch\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Load the model for binary classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',               # Directory to save model checkpoints\n",
    "    num_train_epochs=2,                 # Number of epochs\n",
    "    per_device_train_batch_size=2,      # Batch size per device\n",
    "    per_device_eval_batch_size=2,       # Batch size for evaluation\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",              # Save model at each epoch\n",
    "    logging_dir='logs',                 # Directory for logs\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    load_best_model_at_end=True,        # Load the best model at the end of training\n",
    "    seed=42                             # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                            # The model you defined\n",
    "    args=training_args,                     # The training arguments\n",
    "    train_dataset=tokenized_dataset['train'],  # The tokenized training dataset\n",
    "    eval_dataset=tokenized_dataset['validation'],  # The tokenized validation dataset\n",
    "    compute_metrics=compute_metrics         # The metrics function\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffde9a-0395-4b90-a173-387d12885417",
   "metadata": {},
   "source": [
    "Step 8: Evaluate the Model\n",
    "Markdown Explanation:\n",
    "\n",
    "After training, evaluate the model on the validation dataset to check its performanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f8e337-4fae-4f2d-938a-277ab6a7604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.5866236090660095, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 0.3026, 'eval_samples_per_second': 16.524, 'eval_steps_per_second': 9.915, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1b08b-fc09-4bf4-8b9c-26f1f6ab93c0",
   "metadata": {},
   "source": [
    "Step 9: Save the Fine-Tuned Model\n",
    "Markdown Explanation:\n",
    "\n",
    "Save the trained model locally for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d97ec7-0a74-467c-afc0-206ecb2cd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"fine_tuned_distilbert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec5a4b-fcb7-4031-8a5b-60cfc8a27571",
   "metadata": {},
   "source": [
    "Step 10: Perform Inference on New Text\n",
    "Markdown Explanation:\n",
    "\n",
    "Test the fine-tuned model with new sentences to predict their sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fe6992-1290-48e6-beb0-f74bf4cef456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer again\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3006de73-d5f6-4cc7-b5c5-e3ff3f19a89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_distilbert\\\\tokenizer_config.json',\n",
       " 'fine_tuned_distilbert\\\\special_tokens_map.json',\n",
       " 'fine_tuned_distilbert\\\\vocab.txt',\n",
       " 'fine_tuned_distilbert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model manually using PyTorch\n",
    "torch.save(model.state_dict(), 'fine_tuned_distilbert/pytorch_model.bin')\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('fine_tuned_distilbert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28229016-6bc3-4c73-8b24-8cc79adbf253",
   "metadata": {},
   "source": [
    "Perform Inference on New Text\n",
    "Markdown Explanation:\n",
    "\n",
    "Test the fine-tuned model with new sentences to predict their sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed04d5d7-d9e8-4c54-acbc-a62f123bc5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Positive\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model for inference\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"fine_tuned_distilbert\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"fine_tuned_distilbert\")\n",
    "\n",
    "# Test with new text\n",
    "text = \"I absolutely love this product!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Print prediction\n",
    "print(\"Predicted Label:\", \"Positive\" if predicted_label == 1 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab018717-75a6-4c1e-9227-4bdf07c1da29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637daeee-b170-475b-8dd5-589c296af2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
